Michael:
Looks like a fun project with some nice practicality! Here are a few comments:
It is clear that this project is also being used for an Autonomous Robots project. After speaking with the instructors, they request:
Getting consent with the other course’s instructor 
In the final report, an explanation on how your write-up will differ from the other class.
I’m assuming there will be more details in the final report such as the specific RL algorithm used. It would probably make most sense to use a policy gradient method which you said might be used.
It sounds like you’re considering using a CNN to encode the occupancy grids. How will this be trained? I’m wondering if it would be before or during RL.
Is RL being used in simulation or in the real-world here? I assume the primary motivation for RL here is to handle dynamic environments which may be harder to model through simulation. It may be harder to train and take longer in the real-world of course. 
If we don’t care about dynamic environments since it is handled by the goal navigation policy, then why don’t we just use a search/greedy algorithm for this? It sounds like this will be included as a baseline so I’m curious as to how it will compare. 
One approach would be to consider the plane as a set of vertices, then treating this as a traveling salesman problem and using a heuristic algorithm for it.
Is RL learning a path for a single map, or how to generalize to many maps? I assume it is just learning to cover the first floor. But it may be interesting to see if it improves by learning how to cover across a set of maps.
The literature review itself looks good! It’s clear that you did a thorough job looking through what might be relevant for this problem. I am curious as to how this navigation stack with a lower-level policy handling goal navigation and a higher-level policy determining goals would fare against an end-to-end navigation policy, but it’s definitely out of scope.

Good luck with the rest of the project!

Best,
Michael




Haoran:
You have made a good progress! Some suggestions for your project to become stronger:
It is better for you to specify which policy gradient algorithm you will use in the final project


Haoran Xu

